# Decentralized federated learning using tensorflow

##  Introduction
Federated Learning offers a creative way to bring these fragmented data sets together for machine learning models, regardless of where they are located and, more crucially, without breaking any privacy rules. As a general rule, FL takes the model to the data for training rather than the other way around. All that is required is for the data-hosting device to be willing to commit to the federation process.The foundation of the FL architecture is a curator or server that controls the training processes from a central location. The majority of clients are edge devices, which could number in the millions. Per training iteration, these devices talk to the server at least twice. They first receive the weights for the current global model from the server, train it using their individual local datasets to produce updated parameters, and then send those updated parameters back to the server for aggregation. Until an accuracy requirement or a predetermined epoch number is met, this communication cycle continues. Aggregation simply refers to an averaging operation in the Federated Averaging Algorithm. The training of a FL model is completed in this manner.Let's build one from scratch in Tensorflow and train it using the MNIST data set from Kaggle now that we have a clear understanding of what FL is and how it functions.
